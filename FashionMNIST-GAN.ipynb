{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a4524c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(41)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d543a2",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e26d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "##check if cuda is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", device)\n",
    "\n",
    "## if cuda is available, get GPU-name for double-check\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "\n",
    "#learning rate\n",
    "lr = 0.0001\n",
    "#beta1 and beta2 for adam optimizer\n",
    "#pytorch default coeff. / recommended values\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "#batchsize, noise dimens. and epochs\n",
    "batchsize = 128\n",
    "noise_dim = 64\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02abff8c",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4322042",
   "metadata": {},
   "outputs": [],
   "source": [
    "#download Fashion-MNIST Dataset\n",
    "#performing transformation on the image data\n",
    "from torchvision import datasets, transforms as T\n",
    "\n",
    "#transform dataset from (h,c,w) into (c,h,w)\n",
    "transform = T.Compose([\n",
    "    T.ToTensor()\n",
    "])\n",
    "#loading training and test datasets\n",
    "training_set = datasets.FashionMNIST(root='Fashion_MNIST/', train=True, download=True, transform=transform )\n",
    "test_set = datasets.FashionMNIST(root = 'Fashion_MNIST/', train=False, download=True,transform=transform)\n",
    "\n",
    "#num of training and testdata\n",
    "print(\"Total number of trainingset:\", len(training_set))\n",
    "print(\"Total number of testset:\" ,len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae50340",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the image and label of it\n",
    "image, label = training_set[7000]\n",
    "print(\"Label of the showed image is:\",label)\n",
    "###squeeze method is used to remove single-dimensional entries from the shape of an array\n",
    "plt.imshow(image.squeeze(), cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90158bf1",
   "metadata": {},
   "source": [
    "# Load Dataset into Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4a9e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2929e3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(training_set, batch_size=batchsize, shuffle=True)\n",
    "##check the number of total batches\n",
    "#it should be 469; 60000 images / 128 images per epoch = 469\n",
    "#it depends on the batchsize number, in this tutorial batchsize = 128\n",
    "print(\"Batches in trainloader:\", len(trainloader))\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, label = next(dataiter)\n",
    "#shape of the images; (batchsize, channel, hight, width)\n",
    "print(images.shape)\n",
    "\n",
    "#function to show 16 images\n",
    "#input: images, number of images that will be displayed\n",
    "\n",
    "def show_images(images, number_images=16):\n",
    "    \n",
    "    # if device is gpu, we have to move tensor to cpu:\n",
    "    #img_cpu = images.detach().cpu()\n",
    "    #img_mesh = make_grif(img_cpu[:number_images], nrow = 4)\n",
    "    \n",
    "    #if device is cpu:\n",
    "    img_mesh = make_grid(images[:number_images], nrow=4)\n",
    "    plt.imshow(img_mesh.permute(1, 2, 0).squeeze())\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8169181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(images, number_images=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0654974f",
   "metadata": {},
   "source": [
    "# Discriminator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dde9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic building block for neural networks\n",
    "from torch import nn\n",
    "#summarize the network\n",
    "from torchsummary import summary\n",
    "#Relu function\n",
    "from torch.nn.modules.activation import LeakyReLU\n",
    "from torch.nn.modules.batchnorm import BatchNorm2d\n",
    "from torch.nn.modules.flatten import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4864ebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_network(in_channels, out_channels, kernel_size, stride):\n",
    "  return nn.Sequential(\n",
    "      nn.Conv2d(in_channels, out_channels, kernel_size, stride),\n",
    "      nn.BatchNorm2d(out_channels),\n",
    "      nn.LeakyReLU(0.2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1461b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    super(Discriminator, self).__init__()\n",
    "    # in channels, out channels, kernelsize and stride\n",
    "    #more infos: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "    self.block_1 = discriminator_network(1, 16, (3,3), 2)\n",
    "    self.block_2 = discriminator_network(16, 32, (5,5), 2)\n",
    "    self.block_3 = discriminator_network(32, 64, (5,5), 2)\n",
    "    self.flatten = nn.Flatten()\n",
    "    self.linear = nn.Linear(in_features = 64, out_features=1)\n",
    "\n",
    "  def forward(self, images):\n",
    "\n",
    "    x1 = self.block_1(images)\n",
    "    x2 = self.block_2(x1)\n",
    "    x3 = self.block_3(x2)\n",
    "\n",
    "    x4 = self.flatten(x3)\n",
    "    x5 = self.linear(x4)\n",
    "\n",
    "    return x5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c176020",
   "metadata": {},
   "outputs": [],
   "source": [
    "Disc = Discriminator()\n",
    "Disc.to(device)\n",
    "\n",
    "summary(Disc, input_size=(1,28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f021173a",
   "metadata": {},
   "source": [
    "# Generator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563c487b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_network(in_channels, out_channels, kernel_size, stride, final_block = False):\n",
    "  if final_block == True:\n",
    "    return nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride ),\n",
    "        nn.Tanh()\n",
    "    )\n",
    "  return nn.Sequential(\n",
    "      nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride),\n",
    "      nn.BatchNorm2d(out_channels),\n",
    "      nn.ReLU() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155a36e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "  def __init__(self, noise_dim):\n",
    "    super(Generator, self).__init__()\n",
    "\n",
    "    self.noise_dim = noise_dim\n",
    "    self.block_1 = generator_network(noise_dim, 256, (3,3), 2)\n",
    "    self.block_2 = generator_network(256,128, (4,4), 1)\n",
    "    self.block_3 = generator_network(128, 64, (3,3), 2)\n",
    "    self.block_4 = generator_network(64,1, (4,4), 2, final_block=True)\n",
    "\n",
    "  def forward(self, r_noise_vec):\n",
    "    ##shape of r_noise is: (batch_size, noise)-> (batch_size, noise_dim, 1, 1)\n",
    "    x = r_noise_vec.view(-1, self.noise_dim,1,1)\n",
    "    x1 = self.block_1(x)\n",
    "    x2 = self.block_2(x1)\n",
    "    x3 = self.block_3(x2)\n",
    "    x4 = self.block_4(x3)\n",
    "\n",
    "    return x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e52f796",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gen = Generator(noise_dim)\n",
    "Gen.to(device)\n",
    "\n",
    "summary(Gen, input_size =(1, noise_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf28bef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Random initialized weights to Normal weights\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca64037",
   "metadata": {},
   "outputs": [],
   "source": [
    "Disc = Disc.apply(weights_init)\n",
    "Gen = Gen.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d47d1a4",
   "metadata": {},
   "source": [
    "# Loss Function and Load Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772d251b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#two loses: real loss and fake loss\n",
    "def real_loss(disc_pred):\n",
    "  criterion = nn.BCEWithLogitsLoss()\n",
    "  ground_truth = torch.ones_like(disc_pred)\n",
    "  loss = criterion(disc_pred, ground_truth)\n",
    "  return loss\n",
    "\n",
    "def fake_loss(disc_pred):\n",
    "  criterion = nn.BCEWithLogitsLoss()\n",
    "  ground_truth = torch.zeros_like(disc_pred)\n",
    "  loss = criterion(disc_pred, ground_truth)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66696df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Disc_opt = torch.optim.Adam(Disc.parameters(), lr = lr, betas=(beta1, beta2))\n",
    "Gen_opt = torch.optim.Adam(Gen.parameters(), lr = lr, betas=(beta1, beta2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424402c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "\n",
    "  total_disc_loss = 0.0\n",
    "  total_gen_loss = 0.0\n",
    "\n",
    "  for real_img, _ in tqdm(trainloader):\n",
    "\n",
    "    real_img = real_img.to(device)\n",
    "    noise = torch.randn(batchsize, noise_dim, device = device)\n",
    "\n",
    "    #find loss and update weights for Discriminator\n",
    "    Disc_opt.zero_grad()\n",
    "\n",
    "    fake_img = Gen(noise)\n",
    "    D_pred = Disc(fake_img)\n",
    "    D_fake_loss = fake_loss(D_pred)\n",
    "\n",
    "    D_pred = Disc(real_img)\n",
    "    D_real_loss = real_loss(D_pred)\n",
    "\n",
    "    D_loss = (D_fake_loss + D_real_loss)/2\n",
    "    total_disc_loss += D_loss.item()\n",
    "\n",
    "    D_loss.backward()\n",
    "    Disc_opt.step()\n",
    "\n",
    "    #find loss and update weights for Gen\n",
    "    Gen_opt.zero_grad()\n",
    "    noise = torch.randn(batchsize, noise_dim, device= device)\n",
    "\n",
    "    fake_img = Gen(noise)\n",
    "    D_pred = Disc(fake_img)\n",
    "    G_loss = real_loss(D_pred)\n",
    "    total_gen_loss += G_loss.item()\n",
    "    G_loss.backward()\n",
    "    Gen_opt.step()\n",
    "\n",
    "  avg_disc_loss = total_disc_loss / len(trainloader)\n",
    "  avg_gen_loss = total_gen_loss / len(trainloader)\n",
    "\n",
    "  print(\"Epoch: {} | Disc_loss: {} | Gen_loss: {}\".format(i+1, avg_disc_loss, avg_gen_loss))\n",
    "\n",
    "  show_images(fake_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a09863",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
